# Because we have three haproxy nodes, we need
# to one active LB IP, and we use keepalived for that.
# These variables must be defined when external_lb_vip_address or
# internal_lb_vip_address is set to FQDN.
## Load Balancer Configuration (haproxy/keepalived)
haproxy_keepalived_external_vip_cidr: "172.21.0.100/32"
haproxy_keepalived_internal_vip_cidr: "172.20.0.100/32"
haproxy_keepalived_external_interface: enp1s0
haproxy_keepalived_internal_interface: br-mgmt

## Ceph cluster fsid (must be generated before first run)
## Generate a uuid using: python -c 'import uuid; print(str(uuid.uuid4()))'
generate_fsid: false
fsid: ac7f2119-d953-49c2-a64a-181f2cb4a6e2

## ceph-ansible settings
## See https://github.com/ceph/ceph-ansible/tree/master/group_vars for
## additional configuration options available.
monitor_address_block: "{{ cidr_networks.storage }}"
public_network: "{{ cidr_networks.storage }}"
cluster_network: "{{ cidr_networks.storage }}"
journal_size: 10240 # size in MB
# ceph-ansible automatically creates pools & keys for OpenStack services
openstack_config: true
cinder_ceph_client: cinder
glance_ceph_client: glance
glance_default_store: rbd
glance_rbd_store_pool: images
nova_libvirt_images_rbd_pool: vms

cinder_backends:
  rbd_volumes:
    volume_driver: cinder.volume.drivers.rbd.RBDDriver
    rbd_pool: volumes
    rbd_ceph_conf: /etc/ceph/ceph.conf
    rbd_store_chunk_size: 8
    volume_backend_name: rbddriver
    rbd_user: "{{ cinder_ceph_client }}"
    rbd_secret_uuid: "{{ cinder_ceph_client_uuid }}"
    report_discard_supported: true

_infrastructure_hosts: &infrastructure_hosts
  infra1:
    ip: 172.20.2.3
  infra2:
    ip: 172.20.2.4
  infra3:
    ip: 172.20.2.13

# nova hypervisors
compute_hosts: &compute_hosts
  compute1:
    ip: 172.20.2.3
  compute2:
    ip: 172.20.2.4
  compute3:
    ip: 172.20.2.5
  compute4:
    ip: 172.20.2.9

ceph-osd_hosts:
  osd1:
    ip: 172.20.32.15
  osd2:
    ip: 172.20.32.16
  osd3:
    ip: 172.20.32.17
